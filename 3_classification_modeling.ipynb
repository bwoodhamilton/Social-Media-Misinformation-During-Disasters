{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n",
    "- [Decision Tree](#Decision-Tree)\n",
    "- [Bagging Classifier](#Bagging-Classifier)\n",
    "- [Random Forest](#Random-Forest)\n",
    "- [Extra Trees](#Extra-Trees)\n",
    "- [AdaBoostClassifier](#AdaBoost-Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries needed for modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>times_retweeted</th>\n",
       "      <th>times_favorited</th>\n",
       "      <th>bot_rating</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-28 16:04:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>AMPMUZIC #CaliforniaFires #californiawildfires...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-12 03:06:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>dwatchnews nam   Rebirth, angst and the 'new n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-03 20:10:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036578</td>\n",
       "      <td>WaterSolarWind   Trump melts down on Pelosi du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-26 08:48:42+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097414</td>\n",
       "      <td>BombayHeadlines #CaliforniaWildfire #californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-02 21:57:37+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>studentveronica   California Wildfires Signal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  times_retweeted  times_favorited  bot_rating  \\\n",
       "0  2019-10-28 16:04:00+00:00                0                1    0.005364   \n",
       "1  2019-11-12 03:06:00+00:00                2                1    0.014544   \n",
       "2  2019-11-03 20:10:28+00:00                0                0    0.036578   \n",
       "3  2019-10-26 08:48:42+00:00                2                2    0.097414   \n",
       "4  2019-11-02 21:57:37+00:00                1                1    0.008751   \n",
       "\n",
       "                                               words  \n",
       "0  AMPMUZIC #CaliforniaFires #californiawildfires...  \n",
       "1  dwatchnews nam   Rebirth, angst and the 'new n...  \n",
       "2  WaterSolarWind   Trump melts down on Pelosi du...  \n",
       "3  BombayHeadlines #CaliforniaWildfire #californi...  \n",
       "4  studentveronica   California Wildfires Signal ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in second CSV file and saving it to a dataframe\n",
    "twitter = pd.read_csv(\"./data/twitter_preprocessed_all.csv\")\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28106, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe rows by columns\n",
    "twitter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date               0\n",
       "times_retweeted    0\n",
       "times_favorited    0\n",
       "bot_rating         1\n",
       "words              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "twitter.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values permanently (We can take this out now)\n",
    "twitter.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date               0\n",
       "times_retweeted    0\n",
       "times_favorited    0\n",
       "bot_rating         0\n",
       "words              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see null values dropped from dataframe\n",
    "twitter.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the datatype for \"bot_rating\" from object to float64\n",
    "twitter['bot_rating'] = pd.to_numeric(twitter['bot_rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28105 entries, 0 to 28105\n",
      "Data columns (total 5 columns):\n",
      "date               28105 non-null object\n",
      "times_retweeted    28105 non-null int64\n",
      "times_favorited    28105 non-null int64\n",
      "bot_rating         28105 non-null float64\n",
      "words              28105 non-null object\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Confirming that datatype of \"bot_rating\" was correctly changed\n",
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column in the dataframe where each \"bot_rating\" that is greater than .5 == True and those that aren't\n",
    "# == False\n",
    "twitter[\"likely_bot\"] = twitter[\"bot_rating\"] > .50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>times_retweeted</th>\n",
       "      <th>times_favorited</th>\n",
       "      <th>bot_rating</th>\n",
       "      <th>words</th>\n",
       "      <th>likely_bot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-28 16:04:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>AMPMUZIC #CaliforniaFires #californiawildfires...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-12 03:06:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>dwatchnews nam   Rebirth, angst and the 'new n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-03 20:10:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036578</td>\n",
       "      <td>WaterSolarWind   Trump melts down on Pelosi du...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-26 08:48:42+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097414</td>\n",
       "      <td>BombayHeadlines #CaliforniaWildfire #californi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-02 21:57:37+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>studentveronica   California Wildfires Signal ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  times_retweeted  times_favorited  bot_rating  \\\n",
       "0  2019-10-28 16:04:00+00:00                0                1    0.005364   \n",
       "1  2019-11-12 03:06:00+00:00                2                1    0.014544   \n",
       "2  2019-11-03 20:10:28+00:00                0                0    0.036578   \n",
       "3  2019-10-26 08:48:42+00:00                2                2    0.097414   \n",
       "4  2019-11-02 21:57:37+00:00                1                1    0.008751   \n",
       "\n",
       "                                               words  likely_bot  \n",
       "0  AMPMUZIC #CaliforniaFires #californiawildfires...       False  \n",
       "1  dwatchnews nam   Rebirth, angst and the 'new n...       False  \n",
       "2  WaterSolarWind   Trump melts down on Pelosi du...       False  \n",
       "3  BombayHeadlines #CaliforniaWildfire #californi...       False  \n",
       "4  studentveronica   California Wildfires Signal ...       False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the creation of the new column and boolean variables\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    26441\n",
       "True      1664\n",
       "Name: likely_bot, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the values for the new column that we created, \"likely_bot\", and how many of each value there are \n",
    "# in the column\n",
    "twitter[\"likely_bot\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05920654687777976"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the mean value of likely bots\n",
    "twitter[\"likely_bot\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all likely bots in a dataframe for bootstrapping sampling\n",
    "twitter_bots_likely = twitter.loc[twitter[\"likely_bot\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>times_retweeted</th>\n",
       "      <th>times_favorited</th>\n",
       "      <th>bot_rating</th>\n",
       "      <th>words</th>\n",
       "      <th>likely_bot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-28 16:04:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>AMPMUZIC #CaliforniaFires #californiawildfires...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-12 03:06:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>dwatchnews nam   Rebirth, angst and the 'new n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-03 20:10:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036578</td>\n",
       "      <td>WaterSolarWind   Trump melts down on Pelosi du...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-26 08:48:42+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097414</td>\n",
       "      <td>BombayHeadlines #CaliforniaWildfire #californi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-02 21:57:37+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>studentveronica   California Wildfires Signal ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  times_retweeted  times_favorited  bot_rating  \\\n",
       "0  2019-10-28 16:04:00+00:00                0                1    0.005364   \n",
       "1  2019-11-12 03:06:00+00:00                2                1    0.014544   \n",
       "2  2019-11-03 20:10:28+00:00                0                0    0.036578   \n",
       "3  2019-10-26 08:48:42+00:00                2                2    0.097414   \n",
       "4  2019-11-02 21:57:37+00:00                1                1    0.008751   \n",
       "\n",
       "                                               words  likely_bot  \n",
       "0  AMPMUZIC #CaliforniaFires #californiawildfires...       False  \n",
       "1  dwatchnews nam   Rebirth, angst and the 'new n...       False  \n",
       "2  WaterSolarWind   Trump melts down on Pelosi du...       False  \n",
       "3  BombayHeadlines #CaliforniaWildfire #californi...       False  \n",
       "4  studentveronica   California Wildfires Signal ...       False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming \n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the True/False values in the \"likely_bot\" column to integer values\n",
    "twitter['likely_bot'] = twitter[\"likely_bot\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28105 entries, 0 to 28105\n",
      "Data columns (total 6 columns):\n",
      "date               28105 non-null object\n",
      "times_retweeted    28105 non-null int64\n",
      "times_favorited    28105 non-null int64\n",
      "bot_rating         28105 non-null float64\n",
      "words              28105 non-null object\n",
      "likely_bot         28105 non-null int64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28105, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the the \n",
    "twitter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bootstrap sample of the dataframe and setting it equal to a variable, boot\n",
    "boot = resample(twitter_bots_likely, replace=True, n_samples=25000, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe out of the original dataframe and the bootstrapped sample\n",
    "twitter_bootstrapped = pd.concat([boot, twitter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    26664\n",
       "0    26441\n",
       "Name: likely_bot, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the values for the \"likely_bot\" column and how many of each value there are\n",
    "twitter_bootstrapped[\"likely_bot\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = twitter_bootstrapped[\"words\"]\n",
    "y = twitter_bootstrapped[\"likely_bot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.502071\n",
       "0    0.497929\n",
       "Name: likely_bot, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of our baseline model\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                               frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a pipeline\n",
    "pipe1 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('nb', MultinomialNB())\n",
    "                ])\n",
    "# Setting the parameters of the pipeline \n",
    "pipe_params1 = {\n",
    "    'tfidf__max_features': [100, 1000, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words' : [None, stop_words.ENGLISH_STOP_WORDS],\n",
    "}\n",
    "# Instantiated the grid search\n",
    "gs1 = GridSearchCV(pipe1, \n",
    "                  param_grid=pipe_params1\n",
    "                 ) \n",
    "# Fitting the model\n",
    "gs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853820639790675"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as the model \n",
    "gs_model = gs1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9009490810485086"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the training score\n",
    "gs_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8902613542215863"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the testing score \n",
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The Naive Bayes is often a good model for NLP, but in this case it is outperformed by other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up a pipeline for Decision tree and tfidf Vectorizer\n",
    "pipe2 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                ])\n",
    "# I removed tfidf feature options so I could try more dt hyperparameters since there has been a lot of\n",
    "# consistency with hyperparametes that work best\n",
    "pipe_params2 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'dt__max_depth': [3, 10],\n",
    "    'dt__min_samples_split': [5, 20],\n",
    "    'dt__min_samples_leaf': [2, 7]\n",
    "}\n",
    "# Instantiated grid search\n",
    "gs2 = GridSearchCV(pipe2, \n",
    "                  param_grid=pipe_params2) \n",
    "# Fitting the model\n",
    "gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364164081527281"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to be the model\n",
    "gs_model2 = gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439991965451441"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score for this model \n",
    "gs_model2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7346539127815018"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score for this model \n",
    "gs_model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The Decision Tree/TFIDF Vectorizer model performed stronger than its counterpart. It shows signs of overfitting, however, the score between train and test is not wide enough to draw any conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the pipeline for a bagging classifier \n",
    "pipe3 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('bag', BaggingClassifier())\n",
    "                ])\n",
    "# Setting the parameters\n",
    "pipe_params3 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'bag__max_samples' : [.5, 1.0, 10],\n",
    "    'bag__n_estimators' : [2, 6, 10]\n",
    "}\n",
    "# Instantiated the grid search\n",
    "gs3 = GridSearchCV(pipe3, \n",
    "                  param_grid=pipe_params3) \n",
    "# Fitting the model to the data\n",
    "gs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9751932404495628"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score\n",
    "gs3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to the best estimator \n",
    "gs_model3 = gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99671085668374"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977781125254199"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The Bagging Classifier/TFIDF Vectorizer model was one of the strongest models that was fit and tested. It shows signs of overfitting, however, the score between train and test is not wide enough to draw any conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                         'tfidf__stop_words': [frozenset({'a', 'about', 'above',\n",
       "                                                          'across', 'after',\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline for random forest \n",
    "pipe4 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('rf', RandomForestClassifier())\n",
    "                ])\n",
    "# Pipeline parameters\n",
    "pipe_params4 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__max_depth': [None, 5, 6]\n",
    "}\n",
    "# Instantiating a grid search\n",
    "gs4 = GridSearchCV(pipe4, \n",
    "                  param_grid=pipe_params4) \n",
    "# Fitting my model\n",
    "gs4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877974896133426"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best score for this model\n",
    "gs4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as this model\n",
    "gs_model4 = gs4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993974088580898"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model4.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911877683211568"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score \n",
    "gs_model4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The Random Forest/TFIDF Vectorizer model performed outstandingly, in fact, it perform second best out of every model that was fit and tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                                          'afterwards', 'again',\n",
       "                                                          'against', 'all',\n",
       "                                                          'almost', 'alone',\n",
       "                                                          'along', 'already',\n",
       "                                                          'also', 'although',\n",
       "                                                          'always', 'am',\n",
       "                                                          'among', 'amongst',\n",
       "                                                          'amoungst', 'amount',\n",
       "                                                          'an', 'and',\n",
       "                                                          'another', 'any',\n",
       "                                                          'anyhow', 'anyone',\n",
       "                                                          'anything', 'anyway',\n",
       "                                                          'anywhere', ...})],\n",
       "                         'xt__max_depth': [None, 5, 6],\n",
       "                         'xt__n_estimators': [100, 150]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline for tfidf and extra trees\n",
    "pipe5 = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('xt', ExtraTreesClassifier())\n",
    "                ])\n",
    "# Setting the pipeline parameters\n",
    "pipe_params5 = {\n",
    "    'tfidf__max_features': [10000],\n",
    "    'tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__stop_words' : [stop_words.ENGLISH_STOP_WORDS],\n",
    "    'xt__n_estimators': [100, 150],\n",
    "    'xt__max_depth': [None, 5, 6]\n",
    "}\n",
    "# Instantiating the grid search\n",
    "gs5 = GridSearchCV(pipe5, \n",
    "                  param_grid=pipe_params5) \n",
    "# Fitting the model\n",
    "gs5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9920658876811508"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best score \n",
    "gs5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator as the model \n",
    "gs_model5 = gs5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993974088580898"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score \n",
    "gs_model5.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955562250508398"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score\n",
    "gs_model5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The Extra Trees/TFIDF Vectorizer model was the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=frozenset(...\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('ada',\n",
       "                                        AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                           base_estimator=None,\n",
       "                                                           learning_rate=1.0,\n",
       "                                                           n_estimators=50,\n",
       "                                                           random_state=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'tfidf__max_df': (0.25, 0.5, 0.75)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pipeline \n",
    "pipe6 = Pipeline([('tfidf', TfidfVectorizer(max_features=10000, \n",
    "                                           ngram_range=(1, 1), \n",
    "                                           stop_words=stop_words.ENGLISH_STOP_WORDS)),\n",
    "                     ('ada', AdaBoostClassifier()),\n",
    "])\n",
    "# Setting the pipeline parameters\n",
    "pipe_params6 = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "}\n",
    "# Instantiated a grid search\n",
    "gs6 = GridSearchCV(pipe6, \n",
    "                  param_grid=pipe_params6) \n",
    "# Fitting the model\n",
    "gs6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7051822316407821"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best score for the model \n",
    "gs6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best estimator to be the model\n",
    "gs_model6 = gs6.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000853670784373"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training score\n",
    "gs_model6.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6989530767492657"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the testing score\n",
    "gs_model6.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions about this model:\n",
    "\n",
    "The AdaBoostClassifier/TFIDF Vectorizer model performed well, but not nearly as well as Extra Trees and Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
